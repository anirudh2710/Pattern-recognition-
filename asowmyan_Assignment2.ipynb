{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asowmyan_Assignment2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "_jVRL_tF6ROu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References : From work under Prof. Mingchen Gao,\n",
        "https://scikit-learn.org/stable/\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "20VAFORDppSj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj03hhZhyCEk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn import svm,metrics\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.keras.datasets.mnist\n",
        "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
        "train_x = train_x / 255.0\n",
        "test_x = test_x / 255.0"
      ],
      "metadata": {
        "id": "EVeKm0MmyGIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = train_x.shape[-1]\n",
        "#print(img_size)"
      ],
      "metadata": {
        "id": "385AnJoG53mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.reshape((-1, img_size*img_size))\n",
        "test_x = test_x.reshape((-1, img_size*img_size))"
      ],
      "metadata": {
        "id": "TranX_zz5uvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = svm.SVC(kernel='linear', gamma='scale')\n",
        "classifier.fit(train_x,train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0-xAQCCzVdR",
        "outputId": "d01ac0b1-b5ec-4234-c7b9-b99748502d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(test_x)"
      ],
      "metadata": {
        "id": "zuK2akxD0ePR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = metrics.accuracy_score(test_y, y_pred)\n",
        "print(\"Accuracy : \"+str(accuracy*100)+'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1ifGsqTySYJ",
        "outputId": "138d5923-c841-4e0c-c8a0-6be2494a4be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 94.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "6qP-C2SO6JDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References : Pattern Classification by Duda, Hart, Stork.\n",
        "https://www.stat.cmu.edu/~ryantibs/convexopt-F15/scribes/11-dual-gen-scribed.pdf"
      ],
      "metadata": {
        "id": "AIYaPpxgp7Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since constraints have inequalities, Lagrangian inequalities are used to find the Lagrange Dual problem from the primal problem which is given.\n",
        "\n",
        "#### Given, $$(x_{1}, y_{1}),.... (x_{N}, y_{N})\\, where \\: \\:  y_{1}, y_{2}, ... y_{N} \\in \\left \\{ -1,1 \\right \\}$$\n",
        "\n",
        "And the optimization problem to be considered is,\n",
        "\n",
        " $$ minimize\\;\\;   W^{T}\\cdot W +\\, C\\sum_{i=1}^{N}\\xi _{i} \\; \\; \\; \\; \\; \\;\\;\\;\\;\\;  \\\\\n",
        "subject\\; to\\;\\;  Y_{i}\\cdot(W^{T} \\cdot X_{i}) \\geq 1-\\xi _{i}  \\\\\n",
        "and \\xi _{i}\\geq 0  \\  for \\ i=1, 2, ...., N\\\\\n",
        "$$\n",
        "\n",
        "#### The lagrange function is given by: \n",
        "\n",
        "$$L(x, \\lambda ) = f(x)+ \\lambda g(x)$$\n",
        "\n",
        "$$\\lambda \\rightarrow \\, Lagrange\\,multiplier$$\n",
        "\n",
        "Similarly, for the case of only one constituent and only two choice variables/classes with optimization problem as,\n",
        "$$maximize \\,f(x,y) \\, subject\\, to: \\, g(x,y)=0 $$\n",
        "$$L(x,y, \\lambda ) = f(x,y)- \\lambda g(x,y)$$\n",
        "\n",
        "#### The claim is to minimize the weighted sum <br>\n",
        "\n",
        "$$\\underset{x}{\\mathrm{min}}\\, \\underset{\\lambda \\geq 0}{\\mathrm{max}} L(x,\\lambda) \\;\\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\;  -  Primal\\; problem$$\n",
        "$$\\underset{\\lambda \\geq 0}{\\mathrm{max}}\\, \\underset{x}{\\mathrm{min}}\\,  L(x,\\lambda) \\;\\; \\; \\; \\; \\; \\; \\; \\; \\; \\;  -  Dual\\; problem$$\n",
        "\n",
        "Hence, the claim is\n",
        "$$\n",
        "\\underset{w,\\xi }{\\mathrm{min}}\\, \\underset{\\mu,\\lambda}{\\mathrm{max}}\\;  L\\; \\geq \\underset{\\mu, \\lambda}{\\mathrm{max}}\\, \\underset{w,\\xi}{\\mathrm{min}}\\;  L\n",
        "$$\n",
        "\n",
        "\n",
        "#### The Lagrange function is:\n",
        "\n",
        "\\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        "\n",
        "$\\alpha \\geq 0$, $ \\beta \\geq 0$, - Langrange multipliers <br>\n",
        "$1 - y_i(w^Tx_i) - \\xi_i \\leq 0$ and $\\xi_i \\geq 0$ for $i=1,...,N$ - inequality constraints <br>\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\\\\n",
        "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_w} = w - \\sum_i \\alpha_i y_i x_i = 0 \\implies w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}\n",
        "  \\\\\n",
        "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_\\xi} = 0 \\implies C - \\alpha_i - \\beta_i = 0\n",
        "  \\\\ \n",
        "  \\implies 0 \\leq \\alpha_i \\leq C \\text{ and } \\beta_i \\geq 0\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\text {Substituting } w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}, C=\\alpha_i + \\beta_i\n",
        "\\text { into Lagrange function, we get the dual problem of maximizing: }\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + (\\alpha_i + \\beta_i) \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\\\= \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_i \\alpha_i - w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} - \\sum_i \\alpha_i \\xi_i - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\\\=\\sum_i \\alpha_i- \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j (\\overrightarrow{x_i}^T x_j )\n",
        "\\end{align}\n",
        "\n",
        "#### Therefore, the primal formulation :\n",
        "$$\n",
        "M = \\frac{1}{\\sqrt{W^T.W \\:+\\: C\\sum_{i=1}^{N}\\xi _{i}}}\\\\\n",
        "$$\n",
        "#### And the dual formulation :\n",
        "$$\n",
        "M = \\frac{1}{\\sqrt{\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\vec{x_{i}}{x_{j}})}}\n",
        "$$"
      ],
      "metadata": {
        "id": "ZNVQX42c6hGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Benefits of Maximing Margin:\n",
        "1. The objective is to maximize the distance(margin) between the closest training point to the plane. It's possible only if we minimize the error. This process increases generalization and reduces overfitting, which wouldn't cause less training errors and high testing errors.\n",
        "2. Haivng large margin proves advantageous in various making classification decisions. For those points near the decision surface, there is 50% probability that classifier can decide any way. For classifiers having large margin, the certainity of a decision is always high.\n",
        "3. Having a large margin makes the classification model robust, which can mean that non-linear relationships can also be modelled.\n",
        "\n",
        "#### Characteristics of support vectors:\n",
        "1. Lagrange multiplier is not zero for those support vector points.\n",
        "2. In linear classification, if x is not on margin, alpha is equal to 0. Alpha is not equal to 0 for those x(support vectors) that lie on margin.\n",
        "3. When using SVM, the number of support vectors for non-linear classification is greater than that in linear classification.\n",
        "4. Support vectors decide the orientation of the hyperplane.\n",
        "5. There are 3 regions where the support vectors might be:\n",
        "    a) On the margin boundaries : w^Tx + b =-1, w^Tx + b=+1(Epsilon=0)\n",
        "    b) Lying within margin region (0<Epsilon<1)\n",
        "    c) On the other side of the hyperplane(Epsilon>=1)\n",
        "\n",
        "#### Benefits of solving dual problem instead for primal problem\n",
        "\n",
        "1. When the number of data points are less than the dimensions, optimization is easier in dual.\n",
        "2. It's the opposite when the number of data points are more.\n",
        "3. The number of parameters depends only on the data points and not on the number of dimensions.\n",
        "4. Data which is non-linearly seperable can be classified in the original feature space.\n",
        "5. Dual problem adapts to the amount of data which is available. Hence, regularizing the sparse support vector in dual problem is more adaptive than the vector of regression coefficients.\n",
        "\n"
      ],
      "metadata": {
        "id": "I0X8008VM8F9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "vmwHtUtH9VbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References : Pattern Classification by Hart, Duda,Stork\n",
        "https://www.stat.cmu.edu/~ryantibs/convexopt-F15/scribes/11-dual-gen-scribed.pdf"
      ],
      "metadata": {
        "id": "3HCDSsenqlhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can reduce one multiclass problem into many binary classification problems.\n",
        "\n",
        "We have a Multiclass problem with the dataset:  $$T = (x_{1}, y_{1}),.... (x_{N}, y_{N})$$\n",
        "\n",
        "\n",
        "\n",
        "#### The lagrange form for the primal problem would be : \n",
        "$$\n",
        "\\underset{W_{m}, \\xi_{i}}{\\mathrm{min}}\\, \\frac{1}{2}\\sum_{m}\\left \\| W_{m} \\right \\|^{2} +C\\sum_{i}\\xi_{i}\\\\\n",
        "Subject \\; to: \\; W_{y_{i}}^{T}\\cdot x_{i}-W_{m_{i}}^{T}\\cdot x_{i}\\; \\geq \\; e_{i}^{m}-\\xi_{i}\\; \\; \\; \\; \\forall m, i\\; \\; \\; \\; \\; \\; \\; \n",
        "$$\n",
        "<br>\n",
        "Where, $$\\;\\;e_{i}^{m} \\; = \\; 1-\\delta _{y_{i},m}$$\n",
        "$$\n",
        "\\delta _{y_{i},m} = \n",
        "\\begin{cases}\n",
        "1 & \\text{ if } y_{i}= m\\\\ \n",
        "0 & \\text{ if } y_{i}\\neq m \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "<br>\n",
        "#### The decision function would be :\n",
        "$$\n",
        "arg\\, \\underset{m}{\\mathrm{max}}\\; W_{m}^{T} \\cdot x \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \n",
        "$$\n",
        "<br>\n",
        "\n",
        "From here, the multiclass problem for dual problem can be written as :\n",
        "$$\n",
        "\\underset{\\alpha}{\\mathrm{min}}\\, f(\\alpha) = \\frac{1}{2}\\sum_{m}\\left \\| W_{m}(\\alpha) \\right \\|^{2} +\\sum_{i}\\sum_{m}e_{i}^{m}\\alpha_{i}^{m} \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\;$$ <br>\n",
        "$$Subject \\; to: \\; (\\alpha_{i}^{m} \\leq C_{i}^{m} \\; \\; \\forall m, \\; \\; \\; \\; \\sum_{m}\\alpha_{i}^{m}=0)\\; \\; \\; \\; \\forall i \\\\\n",
        "$$\n",
        "$$\n",
        "g_{i}^{m} = \\frac{\\partial f(\\alpha)}{\\partial \\alpha_{i}^{m}} = W_{m}(\\alpha)^T\\cdot x_{i} + e_{i}^{m} \\; \\; \\; \\; \\; \\; \\forall i,m \\; \\; \\; \\; \\; \\; \\; \n",
        "$$<br>\n",
        "This optimality can be checked using $V_{i}^{m}$ with below conditions,<br>\n",
        "$$\n",
        "V_{i}^{m} = \n",
        "\\begin{cases}\n",
        "g_{i}^{m} & \\text{ if } 0<\\alpha_{i}^{m}<C \\\\\n",
        "max(0, -g_{i}^{m}) & \\text{ if } \\alpha_{i}^{m}=0 \\\\ \n",
        "max(0, g_{i}^{m}) & \\text{ if } \\alpha_{i}^{m}=C\\\\\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "S5qQ118y9XOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hkyxzmgZ-DIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}